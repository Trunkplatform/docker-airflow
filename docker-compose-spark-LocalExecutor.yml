version: '2.1'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow

    webserver:
        image: ailohq/airflow-pyspark:1.11
        restart: always
        depends_on:
            - postgres
        environment:
            PROJECT_HOME:
            LOAD_EX: n
            EXECUTOR: Local
            AIRFLOW_CONN_SPARK_MASTER: spark://spark-master:7077
            rest_db_url: jdbc:postgresql://192.168.2.237:5432/rest_reports_prod_20180116
            rest_db_user: postgres
            rest_db_password: postgres
            console_db_url: jdbc:postgresql://192.168.2.237:5432/console_reports_prod_20180116
            console_db_user: postgres
            console_db_password: postgres
#        env_file:
#            - ${PROJECT_HOME}/.env
        volumes:
            - ${PROJECT_HOME}/dags:/usr/local/airflow/dags
            - ${PROJECT_HOME}/scripts:/usr/local/airflow/scripts
            - ${PROJECT_HOME}/libs:/usr/local/airflow/libs
        ports:
            - "8080:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    spark-master:
        image: ailohq/zeppelin-pyspark:1.40
        hostname: master
        ports: ['8081:8080']
        environment:
            SPARK_NO_DAEMONIZE: "true"
            service_url: http://localhost:8081
            SPARK_DAEMON_JAVA_OPTS: |
                -Dspark.ui.reverseProxy=true
                -Dspark.ui.reverseProxyUrl=http://localhost:8081
        command: |
            /bin/bash -c "$${SPARK_HOME}/sbin/start-master.sh"

    spark-slave-1:
        container_name: spark-slave-1
        hostname: slave1
        depends_on:
            - spark-master
        image: ailohq/zeppelin-pyspark:1.40
        environment:
            SPARK_NO_DAEMONIZE: "true"
            SPARK_WORKER_CORES: 4
            SPARK_WORKER_MEMORY: 4g
            SPARK_MASTER_URL: spark://spark-master:7077
            SPARK_DAEMON_JAVA_OPTS: |
                -Dspark.ui.reverseProxy=true
                -Dspark.ui.reverseProxyUrl=http://localhost:8081
        command: /start-spark-slave.sh

    spark-slave-2:
        container_name: spark-slave-2
        hostname: slave2
        depends_on:
            - spark-master
        image: ailohq/zeppelin-pyspark:1.40
        environment:
            SPARK_NO_DAEMONIZE: "true"
            SPARK_WORKER_CORES: 4
            SPARK_WORKER_MEMORY: 4g
            SPARK_MASTER_URL: spark://spark-master:7077
            SPARK_DAEMON_JAVA_OPTS: |
              -Dspark.ui.reverseProxy=true
              -Dspark.ui.reverseProxyUrl=http://localhost:8081
        command: /start-spark-slave.sh

    hdfs-namenode:
        image: uhopper/hadoop-namenode
        hostname: namenode
        container_name: namenode
        volumes:
            - ./volumes/dfs/name:/hadoop/dfs/name
        environment:
            CLUSTER_NAME: hadoop
            HDFS_CONF_dfs_permissions: 'false'
        ports:
            - 50070:50070
            - 8020:8020

    hdfs-datanode:
        image: uhopper/hadoop-datanode
        depends_on:
            - hdfs-namenode
        hostname: datanode
        container_name: datanode
        environment:
            CORE_CONF_fs_defaultFS: hdfs://hdfs-namenode:8020
        volumes:
            - ./volumes/dfs/data:/hadoop/dfs/data
