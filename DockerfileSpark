# VERSION 1.9.0-1
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

# FROM python:3.6-slim
FROM openjdk:8u212-jdk-stretch

MAINTAINER Ailo <engineers@ailo.io>

ENV SPARK_VERSION=2.2.1
ENV HADOOP_VERSION=2.9.1

ENV SPARK_HOME=/usr/spark
ADD spark-${SPARK_VERSION}-bin-hadoop-${HADOOP_VERSION}.tgz /
RUN mv /spark-${SPARK_VERSION}-bin-hadoop-${HADOOP_VERSION} $SPARK_HOME
ENV PATH=${SPARK_HOME}/bin:${PATH}

RUN rm $SPARK_HOME/jars/commons-beanutils-1.*

ADD https://jdbc.postgresql.org/download/postgresql-9.4.1212.jar $SPARK_HOME/jars/
ADD http://central.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.199/aws-java-sdk-bundle-1.11.199.jar $SPARK_HOME/jars/
ADD http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.9.1/hadoop-aws-2.9.1.jar $SPARK_HOME/jars/
ADD http://central.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.2/commons-beanutils-1.9.2.jar $SPARK_HOME/jars/

RUN chmod +r $SPARK_HOME/jars/*

# Never prompts the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux
ENV AIRFLOW_GPL_UNIDECODE yes
ENV SLUGIFY_USES_TEXT_UNIDECODE yes

# Airflow
ARG AIRFLOW_VERSION=1.10.5
ARG AIRFLOW_HOME=/usr/local/airflow

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8
ENV LC_ALL en_US.UTF-8

ENV CONDA_HOME /usr/miniconda3

# Install miniconda
RUN apt-get update && \
    apt-get install --no-install-recommends -y bzip2 wget git apt-utils curl rsync netcat locales build-essential vim && \
    sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen && \
    locale-gen && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 && \
    useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow

ADD https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh miniconda3.sh

RUN apt-get update && apt-get install -y python-dev gcc libspatialindex-dev binutils libproj-dev gdal-bin

RUN bash miniconda3.sh -p $CONDA_HOME -b && rm miniconda3.sh
ENV PATH=${CONDA_HOME}/bin:${PATH}
ENV PYSPARK_DRIVER_PYTHON=$CONDA_HOME/bin/python
ENV PYSPARK_PYTHON=$CONDA_HOME/bin/python
RUN conda update -y conda && conda install -y python=3.6
RUN conda install matplotlib

RUN conda install -c conda-forge gdal
RUN conda install -c conda-forge fiona
RUN conda install -c conda-forge geopandas

RUN conda install -c conda-forge pyarrow==0.14.1
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip
RUN pip install pytz pyOpenSSL ndg-httpsclient pyasn1 virtualenv Flask==1.0.4 dask[dataframe]==2.3.0 boto3==1.9.240 \
    apache-airflow[crypto,celery,postgres,hive,jdbc,slack,github_enterprise,ssh]==$AIRFLOW_VERSION

COPY AIRFLOW-3626.patch /AIRFLOW-3626.patch
RUN patch -i AIRFLOW-3626.patch /usr/miniconda3/lib/python3.6/site-packages/airflow/models/__init__.py
COPY requirements.txt /requirements.txt

RUN pip install --no-cache -r /requirements.txt
RUN python -m nltk.downloader punkt
RUN rm -rf /root/.cache && apt-get autoclean && apt-get clean

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_HOME}

EXPOSE 8080 5555 8793

USER airflow
WORKDIR ${AIRFLOW_HOME}
ENTRYPOINT ["/entrypoint.sh"]
