# VERSION 1.9.0-1
# AUTHOR: Matthieu "Puckel_" Roisil
# DESCRIPTION: Basic Airflow container
# BUILD: docker build --rm -t puckel/docker-airflow .
# SOURCE: https://github.com/puckel/docker-airflow

# FROM python:3.6-slim
FROM bde2020/spark-base:2.2.0-hadoop2.7
MAINTAINER Puckel_

ENV SPARK_VERSION=2.2.0
ENV SPARK_HOME=/usr/spark-$SPARK_VERSION
RUN mv /spark $SPARK_HOME
ENV PATH=${SPARK_HOME}/bin:${PATH}

RUN rm $SPARK_HOME/jars/commons-beanutils-1.*

ADD http://central.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar $SPARK_HOME/jars/
ADD http://central.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar $SPARK_HOME/jars/
ADD http://central.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.2/commons-beanutils-1.9.2.jar $SPARK_HOME/jars/

# Never prompts the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.9.0
ARG AIRFLOW_HOME=/usr/local/airflow

# Define en_US.
ENV LANGUAGE en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LC_ALL en_US.UTF-8
ENV LC_CTYPE en_US.UTF-8
ENV LC_MESSAGES en_US.UTF-8
ENV LC_ALL en_US.UTF-8

ENV CONDA_HOME /usr/miniconda3

# Install miniconda
RUN apt-get update && \
    apt-get install -y bzip2 wget git apt-utils curl rsync netcat locales build-essential && \
    sed -i 's/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g' /etc/locale.gen && \
    locale-gen && update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 && \
    useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow &&\
    mkdir $AIRFLOW_HOME/libs &&\
    curl -o $AIRFLOW_HOME/libs/postgresql-9.4-1202.jdbc41.jar https://jdbc.postgresql.org/download/postgresql-9.4-1202.jdbc41.jar

ADD https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh miniconda3.sh

RUN bash miniconda3.sh -p $CONDA_HOME -b && rm miniconda3.sh
ENV PATH=${CONDA_HOME}/bin:${PATH}
ENV PYSPARK_DRIVER_PYTHON=$CONDA_HOME/bin/python
ENV PYSPARK_PYTHON=$CONDA_HOME/bin/python
RUN conda update -y conda
ENV PYTHONPATH $SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip
RUN pip install Cython pytz pyOpenSSL ndg-httpsclient pyasn1 \
    apache-airflow[crypto,celery,postgres,hive,jdbc,slack]==$AIRFLOW_VERSION \
    celery[redis]==4.0.2

COPY requirements.txt /requirements.txt

RUN pip install --no-cache -r /requirements.txt
RUN python -m nltk.downloader punkt
RUN rm -rf /root/.cache && apt-get autoclean && apt-get clean

COPY script/entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_HOME}/airflow.cfg

RUN chown -R airflow: ${AIRFLOW_HOME}

EXPOSE 8080 5555 8793

USER airflow
WORKDIR ${AIRFLOW_HOME}
ENTRYPOINT ["/entrypoint.sh"]
